<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="这里要写啥我不是很清楚"><title>Scrapy笔记02- 完整示例 | 那年八月</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/3.0.3/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><script type="text/javascript" src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Scrapy笔记02- 完整示例</h1><a id="logo" href="/.">那年八月</a><p class="description">奔赴一座城，拥抱一个人 →_→</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa myfa fa-home"></i>首页</a><a href="/archives/"><i class="fa myfa fa-archive"></i>归档</a><a href="/about/"><i class="fa myfa fa-user"></i>关于</a><a href="/guestbook/"><i class="fa myfa fa-comments"></i>留言</a><a href="/atom.xml"><i class="fa myfa fa-rss"></i>订阅</a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Scrapy笔记02- 完整示例</h1><div class="post-meta">Mar 10, 2016<span> | </span><span class="category"><a href="/categories/python/">python</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/03/10/scrapy-02.html" href="/2016/03/10/scrapy-02.html#ds-thread" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#创建Scrapy工程"><span class="toc-number">1.</span> <span class="toc-text">创建Scrapy工程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#定义我们的Item"><span class="toc-number">2.</span> <span class="toc-text">定义我们的Item</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#第一个Spider"><span class="toc-number">3.</span> <span class="toc-text">第一个Spider</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#运行爬虫"><span class="toc-number">4.</span> <span class="toc-text">运行爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#处理链接"><span class="toc-number">5.</span> <span class="toc-text">处理链接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#导出抓取数据"><span class="toc-number">6.</span> <span class="toc-text">导出抓取数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#保存数据到数据库"><span class="toc-number">7.</span> <span class="toc-text">保存数据到数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#下一步"><span class="toc-number">8.</span> <span class="toc-text">下一步</span></a></li></ol></div></div><div class="post-content"><p>这篇文章我们通过一个比较完整的例子来教你使用Scrapy，我选择爬取<a href="http://www.huxiu.com/" target="_blank" rel="external">虎嗅网首页</a>的新闻列表。</p>
<p>这里我们将完成如下几个步骤：</p>
<ul>
<li>创建一个新的Scrapy工程</li>
<li>定义你所需要要抽取的Item对象</li>
<li>编写一个spider来爬取某个网站并提取出所有的Item对象</li>
<li>编写一个Item Pipline来存储提取出来的Item对象</li>
</ul>
<p>Scrapy使用Python语言编写，如果你对这门语言还不熟，请先去学习下基本知识。<a id="more"></a></p>
<h2 id="创建Scrapy工程"><a href="#创建Scrapy工程" class="headerlink" title="创建Scrapy工程"></a>创建Scrapy工程</h2><p>在任何你喜欢的目录执行如下命令
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject coolscrapy</span><br></pre></td></tr></table></figure></p>
<p>将会创建coolscrapy文件夹，其目录结构如下：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">coolscrapy/</span><br><span class="line">    scrapy.cfg            # 部署配置文件</span><br><span class="line"></span><br><span class="line">    coolscrapy/           # Python模块，你所有的代码都放这里面</span><br><span class="line">        __init__.py</span><br><span class="line"></span><br><span class="line">        items.py          # Item定义文件</span><br><span class="line"></span><br><span class="line">        pipelines.py      # pipelines定义文件</span><br><span class="line"></span><br><span class="line">        settings.py       # 配置文件</span><br><span class="line"></span><br><span class="line">        spiders/          # 所有爬虫spider都放这个文件夹下面</span><br><span class="line">            __init__.py</span><br><span class="line">            ...</span><br></pre></td></tr></table></figure></p>
<h2 id="定义我们的Item"><a href="#定义我们的Item" class="headerlink" title="定义我们的Item"></a>定义我们的Item</h2><p>我们通过创建一个scrapy.Item类，并定义它的类型为scrapy.Field的属性，
我们准备将虎嗅网新闻列表的名称、链接地址和摘要爬取下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()    <span class="comment"># 标题</span></span><br><span class="line">    link = scrapy.Field()     <span class="comment"># 链接</span></span><br><span class="line">    desc = scrapy.Field()     <span class="comment"># 简述</span></span><br><span class="line">    posttime = scrapy.Field() <span class="comment"># 发布时间</span></span><br></pre></td></tr></table></figure>
<p>也许你觉得定义这个Item有点麻烦，但是定义完之后你可以得到许多好处，这样你就可以使用Scrapy中其他有用的组件和帮助类。</p>
<h2 id="第一个Spider"><a href="#第一个Spider" class="headerlink" title="第一个Spider"></a>第一个Spider</h2><p>蜘蛛就是你定义的一些类，Scrapy使用它们来从一个domain（或domain组）爬取信息。
在蜘蛛类中定义了一个初始化的URL下载列表，以及怎样跟踪链接，如何解析页面内容来提取Item。</p>
<p>定义一个Spider，只需继承<code>scrapy.Spider</code>类并定于一些属性：</p>
<ul>
<li>name: Spider名称，必须是唯一的</li>
<li>start_urls: 初始化下载链接URL</li>
<li>parse(): 用来解析下载后的Response对象，该对象也是这个方法的唯一参数。
它负责解析返回页面数据并提取出相应的Item（返回Item对象），还有其他合法的链接URL（返回Request对象）。</li>
</ul>
<p>我们在coolscrapy/spiders文件夹下面新建<code>huxiu_spider.py</code>，内容如下：
<figure class="highlight python"><figcaption><span>huxiu_spider.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">Topic: sample</span><br><span class="line">Desc :</span><br><span class="line">"""</span></span><br><span class="line"><span class="keyword">from</span> coolscrapy.items <span class="keyword">import</span> HuxiuItem</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"huxiu"</span></span><br><span class="line">    allowed_domains = [<span class="string">"huxiu.com"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.huxiu.com/index.php"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="mod-info-flow"]/div/div[@class="mob-ctt"]'</span>):</span><br><span class="line">            item = HuxiuItem()</span><br><span class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'h3/a/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            item[<span class="string">'link'</span>] = sel.xpath(<span class="string">'h3/a/@href'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            url = response.urljoin(item[<span class="string">'link'</span>])</span><br><span class="line">            item[<span class="string">'desc'</span>] = sel.xpath(<span class="string">'div[@class="mob-sub"]/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            print(item[<span class="string">'title'</span>],item[<span class="string">'link'</span>],item[<span class="string">'desc'</span>])</span><br></pre></td></tr></table></figure></p>
<h2 id="运行爬虫"><a href="#运行爬虫" class="headerlink" title="运行爬虫"></a>运行爬虫</h2><p>在根目录执行下面的命令，其中huxiu是你定义的spider名字：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl huxiu</span><br></pre></td></tr></table></figure></p>
<p>如果一切正常，应该可以打印出每一个新闻</p>
<h2 id="处理链接"><a href="#处理链接" class="headerlink" title="处理链接"></a>处理链接</h2><p>如果想继续跟踪每个新闻链接进去，看看它的详细内容的话，那么可以在parse()方法中返回一个Request对象，
然后注册一个回调函数来解析新闻详情。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> coolscrapy.items <span class="keyword">import</span> HuxiuItem</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuxiuSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"huxiu"</span></span><br><span class="line">    allowed_domains = [<span class="string">"huxiu.com"</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.huxiu.com/index.php"</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.xpath(<span class="string">'//div[@class="mod-info-flow"]/div/div[@class="mob-ctt"]'</span>):</span><br><span class="line">            item = HuxiuItem()</span><br><span class="line">            item[<span class="string">'title'</span>] = sel.xpath(<span class="string">'h3/a/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            item[<span class="string">'link'</span>] = sel.xpath(<span class="string">'h3/a/@href'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            url = response.urljoin(item[<span class="string">'link'</span>])</span><br><span class="line">            item[<span class="string">'desc'</span>] = sel.xpath(<span class="string">'div[@class="mob-sub"]/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            <span class="comment"># print(item['title'],item['link'],item['desc'])</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse_article)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_article</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        detail = response.xpath(<span class="string">'//div[@class="article-wrap"]'</span>)</span><br><span class="line">        item = HuxiuItem()</span><br><span class="line">        item[<span class="string">'title'</span>] = detail.xpath(<span class="string">'h1/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        item[<span class="string">'link'</span>] = response.url</span><br><span class="line">        item[<span class="string">'posttime'</span>] = detail.xpath(</span><br><span class="line">            <span class="string">'div[@class="article-author"]/span[@class="article-time"]/text()'</span>)[<span class="number">0</span>].extract()</span><br><span class="line">        print(item[<span class="string">'title'</span>],item[<span class="string">'link'</span>],item[<span class="string">'posttime'</span>])</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<p>现在parse只提取感兴趣的链接，然后将链接内容解析交给另外的方法去处理了。
你可以基于这个构建更加复杂的爬虫程序了。</p>
<h2 id="导出抓取数据"><a href="#导出抓取数据" class="headerlink" title="导出抓取数据"></a>导出抓取数据</h2><p>最简单的保存抓取数据的方式是使用json格式的文件保存在本地，像下面这样运行：
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl huxiu -o items.json</span><br></pre></td></tr></table></figure></p>
<p>在演示的小系统里面这种方式足够了。不过如果你要构建复杂的爬虫系统，
最好自己编写<a href="http://doc.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline" target="_blank" rel="external">Item Pipeline</a>。</p>
<h2 id="保存数据到数据库"><a href="#保存数据到数据库" class="headerlink" title="保存数据到数据库"></a>保存数据到数据库</h2><p>上面我们介绍了可以将抓取的Item导出为json格式的文件，不过最常见的做法还是编写Pipeline将其存储到数据库中。我们在<code>coolscrapy/pipelines.py</code>定义
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.orm <span class="keyword">import</span> sessionmaker</span><br><span class="line"><span class="keyword">from</span> coolscrapy.models <span class="keyword">import</span> News, db_connect, create_news_table, Article</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleDataBasePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""保存文章到数据库"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        engine = db_connect()</span><br><span class="line">        create_news_table(engine)</span><br><span class="line">        self.Session = sessionmaker(bind=engine)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="string">"""This method is called when the spider is opened."""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        a = Article(url=item[<span class="string">"url"</span>],</span><br><span class="line">                    title=item[<span class="string">"title"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    publish_time=item[<span class="string">"publish_time"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    body=item[<span class="string">"body"</span>].encode(<span class="string">"utf-8"</span>),</span><br><span class="line">                    source_site=item[<span class="string">"source_site"</span>].encode(<span class="string">"utf-8"</span>))</span><br><span class="line">        <span class="keyword">with</span> session_scope(self.Session) <span class="keyword">as</span> session:</span><br><span class="line">            session.add(a)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<p>上面我使用了python中的SQLAlchemy来保存数据库，这个是一个非常优秀的ORM库，我写了篇关于它的<a href="http://yidao620c.github.io/2016/03/07/sqlalchemy.html">入门教程</a>，可以参考下。</p>
<p>然后在<code>setting.py</code>中配置这个Pipeline，还有数据库链接等信息：
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'coolscrapy.pipelines.ArticleDataBasePipeline'</span>: <span class="number">5</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># linux pip install MySQL-python</span></span><br><span class="line">DATABASE = &#123;<span class="string">'drivername'</span>: <span class="string">'mysql'</span>,</span><br><span class="line">            <span class="string">'host'</span>: <span class="string">'192.168.203.95'</span>,</span><br><span class="line">            <span class="string">'port'</span>: <span class="string">'3306'</span>,</span><br><span class="line">            <span class="string">'username'</span>: <span class="string">'root'</span>,</span><br><span class="line">            <span class="string">'password'</span>: <span class="string">'mysql'</span>,</span><br><span class="line">            <span class="string">'database'</span>: <span class="string">'spider'</span>,</span><br><span class="line">            <span class="string">'query'</span>: &#123;<span class="string">'charset'</span>: <span class="string">'utf8'</span>&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p>再次运行爬虫
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl huxiu</span><br></pre></td></tr></table></figure></p>
<p>那么所有新闻的文章都存储到数据库中去了。</p>
<h2 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h2><p>本章只是带你领略了scrapy最基本的功能，还有很多高级特性没有讲到。接下来会通过多个例子向你展示scrapy的其他特性，然后再深入讲述每个特性。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://yidao620c.github.io/2016/03/10/scrapy-02.html" data-id="cixyqada300260or16u9i82vp" class="article-share-link">分享到</a><div class="tags"><a href="/tags/scrapy/">scrapy</a></div><div class="post-nav"><a href="/2016/03/12/scrapy-03.html" class="pre">上一篇: Scrapy笔记03- Spider详解</a><a href="/2016/03/08/scrapy-01.html" class="next">下一篇: Scrapy笔记01- 入门篇</a></div><div data-thread-key="2016/03/10/scrapy-02.html" data-title="Scrapy笔记02- 完整示例" data-url="http://yidao620c.github.io/2016/03/10/scrapy-02.html" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2016/03/10/scrapy-02.html" data-title="Scrapy笔记02- 完整示例" data-url="http://yidao620c.github.io/2016/03/10/scrapy-02.html" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yidao620c.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa myfa fa-folder-o"></i>分类</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/toolkit/">toolkit</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术随笔/">技术随笔</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/朝花夕拾/">朝花夕拾</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法之美/">算法之美</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa myfa fa-star-o"></i>标签</div><div class="tagcloud"><a href="/tags/redis/" style="font-size: 15px;">redis</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/负载均衡/" style="font-size: 15px;">负载均衡</a> <a href="/tags/笑话/" style="font-size: 15px;">笑话</a> <a href="/tags/love/" style="font-size: 15px;">love</a> <a href="/tags/memcached/" style="font-size: 15px;">memcached</a> <a href="/tags/mqtt/" style="font-size: 15px;">mqtt</a> <a href="/tags/nginx/" style="font-size: 15px;">nginx</a> <a href="/tags/octopress/" style="font-size: 15px;">octopress</a> <a href="/tags/osgi/" style="font-size: 15px;">osgi</a> <a href="/tags/rabbitmq/" style="font-size: 15px;">rabbitmq</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/scrapy/" style="font-size: 15px;">scrapy</a> <a href="/tags/servlet/" style="font-size: 15px;">servlet</a> <a href="/tags/django/" style="font-size: 15px;">django</a> <a href="/tags/sitemesh/" style="font-size: 15px;">sitemesh</a> <a href="/tags/SQLAlchemy/" style="font-size: 15px;">SQLAlchemy</a> <a href="/tags/xpath/" style="font-size: 15px;">xpath</a> <a href="/tags/yaml/" style="font-size: 15px;">yaml</a> <a href="/tags/sed/" style="font-size: 15px;">sed</a> <a href="/tags/python核心/" style="font-size: 15px;">python核心</a> <a href="/tags/jinja2/" style="font-size: 15px;">jinja2</a> <a href="/tags/vagrant/" style="font-size: 15px;">vagrant</a> <a href="/tags/wsgi/" style="font-size: 15px;">wsgi</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/awk/" style="font-size: 15px;">awk</a></div></div><div class="widget"><div class="widget-title"><i class="fa myfa fa-file-o"></i>最新文章</div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/03/07/joke.html">开心一刻</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/22/linux/awk.html">awk命令笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/20/linux/sed.html">sed命令笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/16/toolkit/mysql-ha.html">centos7配置mysql主从复制</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/09/toolkit/gitlab.html">centos7安装gitlab8.9</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/05/python/jinja2.html">jinja2模板</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/02/python/vagrant.html">Vagrant创建虚拟化开发环境</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/08/30/love.html">临汾印象</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/26/python/pycharm-remote.html">使用PyCharm进行远程开发和调试</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/30/python/django-deploy.html">CentOS7上使用mod_wsgi部署Django</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa myfa fa-comment-o"></i>最近评论</div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa myfa fa-external-link"></i>友情链接</div><ul></ul><a href="http://www.huxiu.com/index.php" title="虎嗅网" target="_blank">虎嗅网</a><ul></ul><a href="http://www.infoq.com/cn/" title="InfoQ" target="_blank">InfoQ</a><ul></ul><a href="http://blog.jobbole.com/" title="伯乐在线" target="_blank">伯乐在线</a><ul></ul><a href="http://www.cnblogs.com/" title="博客园" target="_blank">博客园</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <span>2015</span><span class="with-love"><i class="fa fa-heart"></i></span>熊能<br/>
由<a rel="nofollow" target="_blank" href="http://hexo.io/" class="footera">Hexo</a>强力驱动 | 主题<a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo" class="footera">maupassant-hexo</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'yidao620c'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>